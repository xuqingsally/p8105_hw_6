---
title: "p8105_hw3_qx2178"
output: html_document
author: Qing Xu
date: 10/5/2017
---
# Problem 1
## 1.1
```{r_1.1}
library(tidyverse)
library(haven)
library(readxl)
library(janitor)
library(knitr)
library(hexbin)

pulse = read_sas("../data/public_pulse_data.sas7bdat") %>%
        clean_names() %>%
        gather(., key = visit, value = bdi, bdiscore_bl:bdiscore_12m) %>%
        separate(visit, into = c("remove", "visit"), sep = "_") %>%
        select(id, visit, everything(), -remove) %>%
        mutate(visit = replace(visit, visit == "bl", "00m"),
         visit = factor(visit, levels = paste0(c("00", "01", "06", "12"), "m"))) %>%
        arrange(id, visit) %>%
        filter(!is.na(bdi))
pulse$bdi<- as.integer(pulse$bdi)
visit_time<- count(pulse, id)
count(visit_time, n)
```  

There are 1087 subjects with 3469 visit times.  
And the number of subjects with observations at 1, 2, 3, or 4 visits are in the table above. There are 104 subjects have 1 visit, 135 subjects have 2 visits, 297 subjects have 3 visits and 551 subjects have 4 visits.  

## 1.2
```{r_1.2}
pulse %>%
group_by(visit) %>%
summarize(mean_bdi = mean(bdi),
            median_bdi = median(bdi),
            sd_bdi = sd(bdi))
```

## 1.3
```{r_1.3}
ggplot(pulse, aes(x = visit, y = bdi)) + 
geom_boxplot()+
geom_violin(aes(fill = visit), color = "blue", alpha = .5) +
labs(title = "BDI score distribution in four visits") 
```  

Four visits have similar distribution of BDI score especially for 01m,06m and 12m. However, there are some extreme data of BDI score of each visit. 

## 1.4
```{r_1.4}
pulse$visit<- as.numeric(pulse$visit)
ggplot(pulse, aes(x = visit, y = bdi))+
geom_point(aes(color = id), alpha = .3) +
geom_path() +
labs(title = "BDI score trend in four visits of each observation") 
```  

Most people have stable BDI scores which are around 10. However, there are a few people who have changed BDI scores over time. Subjects with high BDI scores at baseline may not have high BDI scores at 12 months.

# Problem 2
## 2.1
```{r_2.1}
instacart<- read_csv("../data/instacart_train_data.csv.zip") %>%
            group_by(department) 
depart<- summarize(instacart,items = n()) %>%
         arrange(.,desc(items)) %>%
         head(7)
kable(depart, caption = "Seven departments from which most items are ordered")
```

## 2.2

```{r_2.2}
order<- group_by(instacart, department, product_name) %>%
        summarize(n_order = n_distinct(order_number)) %>%
        arrange(., department,desc(n_order)) %>%
        filter(min_rank(desc(n_order))< 2)
kable(order, caption = "Most popular items in each department")
```

## 2.3

```{r_2.3}
PC<- filter(instacart, product_name == "Pink Lady Apples"| product_name == "Coffee Ice Cream") %>%
     group_by(product_name, order_dow) %>%
     summarize(mean_hour = mean(order_hour_of_day)) %>%
     spread(key = product_name, value = mean_hour)
     
kable(PC, caption = "Mean order hour of day of Pink Lady Apples and Coffee Ice Cream on each day of the week")
```

## 2.4

```{r_2.4}
library(ggridges)
library(ggthemes)
instacart$department = reorder(instacart$department, instacart$order_hour_of_day, IQR)

ggplot(instacart, aes(x = department, y = order_hour_of_day)) + 
  geom_violin(aes(fill = department), color = "blue", alpha = .5) +
  labs(title = "Order hour of day distribution in each department")
  
ggplot(instacart, aes(x = order_hour_of_day, y = department)) +
  geom_density_ridges(scale = .85) +
  labs(title = "Order hour of day density of each department")
  
iqr<- summarize(instacart, IQR = IQR(order_hour_of_day)) %>%
      arrange(IQR)
```  

Babies,bakery,breakfast,dairy eggs,household and personal care have widest IQR which is 7.  
Alcohol and Canned goods have narrowest IQR which is 5.

#Problem 3
## 3.1
```{r_3.1}
noaa<- read_csv("../data/nynoaadat.zip") %>%
       group_by(id)
obser<- summarize(noaa, n = n())
table(is.na(noaa$snow)) 
table(is.na(noaa$tmax))
Snow<- aggregate(snow ~ id, data = noaa, function(x) {sum(is.na(x))}, na.action = NULL) %>%
    ggplot(aes(x = id, y = snow)) + 
    geom_point(alpha = .3)
    labs(title = "Number of missing snowfall data of each station",
    y = "Number of missing snowfall data"
  )
Snow

Tmax<- aggregate(tmax ~ id, data = noaa, function(x) {sum(is.na(x))}, na.action = NULL) %>%
ggplot(aes(x = id, y = tmax)) + 
    geom_point(alpha = .3) +
    labs(title = "Number of missing tmax of each station",
    y = "Number of missing tmax"
  )
Tmax
```  

There are 2595176 observations and 747 stations.  
There are 381221 missing data of snow and 1134358 missing data of tmax.  
According to plot, it varies by station.  

## 3.2
```{r_3.2}
snowfall<- separate(noaa, date, into = c("year","month","day"),sep = "-") %>%
           filter(!is.na(snow)) %>%
           group_by(year) %>%
           arrange(desc(snow)) %>%
           head(1)
snowfall
```  

1983 had largest snowfall in a single day.    
Yes,On February 10-12, 1983, a blizzard swept up the Eastern Seaboard, burying an area from Virginia to the Northeast U.S. under a blanket of very heavy snow. 

## 3.3
```{r_3.3}
limit<- filter(noaa, snow < 100 & snow > 0) %>%
        separate(date, into = c("year","month","day"),sep = "-") %>%
        group_by(year) 
        
ggplot(limit, aes(x = snow, y = year)) + 
  geom_density_ridges(scale = .85) +
  labs(title = "snowfall density in each year")

```  

The recorded snowfall values are clustered around specific entries such as 10, 25 and 50.  
I think this is because every year New York may have snow in winter which normally have snowfall values as 10 and 25. However, in deep winter there may be 50 snowfall values which have less duration but larger snowfall values.  

## 3.4
```{r_3.4}
tt<- filter(noaa, !is.na(tmax)) %>%
     filter(!is.na(tmin)) %>%
     group_by(id)
tt$tmax<- as.numeric(tt$tmax)
tt$tmin<- as.numeric(tt$tmin)
ggplot(tt, aes(tmax, tmin)) +
  geom_hex() +
  labs(title = "tmax vs tmin in each observation")
```

## 3.5
```{r_3.5}
new_noaa<- separate(noaa, date, into = c("year","month","day"),sep = "-") %>%
           filter(!is.na(tmax)) %>%
           group_by(id, month) 
new_noaa$tmax<- as.numeric(new_noaa$tmax) 
new_noaa<- summarize(new_noaa, ave_tmax = mean(tmax)) 
new_noaa$month<- as.numeric(new_noaa$month)

ggplot(new_noaa, aes(x = month, y = ave_tmax))+
geom_point(alpha = .3) +
geom_path() +
labs( title = "Average tmax of each station during one year")
```  

Most stations have stable average tmax through the year.  
However, there are some stations have highest average tmax during the middle of the year which is summer. Maybe these stations are in the places where have high temperature in summer.